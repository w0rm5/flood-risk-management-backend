{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8447ad3-f7f2-4e92-9270-07be01fdc148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import json\n",
    "import time\n",
    "#filter datasets into one file and then seperate into weather station with all years and times in one\n",
    "weatherStations = [6092, 6098, 6121, 6124, 6127, 6129,6131, 6132, 6133, 6136, 6143, 6003, 6029, 6026, 6062, 6029, 6062, 6087]\n",
    "#weatherStations = map(int, os.environ.get(\"WEATHER_STATIONS\").split(\", \"))\n",
    "#FLOOD_DATA_DIR = os.environ.get(\"FLOOD_DATA_DIR\")\n",
    "\n",
    "\n",
    "def newFillOutData():\n",
    "    #first do pathlib to get all the files\n",
    "    directory = \"./RFbyStation\"\n",
    "    files = list(Path(directory).glob('*'))\n",
    "    for oldFile in files:\n",
    "        if str(oldFile) != \"RFbyStation\\\\6993_2011-2023_60rf.txt\":\n",
    "            df = pd.read_csv(oldFile)\n",
    "\n",
    "            df['obstime'] = pd.to_datetime(df['obstime'], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "            #If you want to fill out the data for all years just change the start to a gen. variable\n",
    "            # of the date time format in the df, this is just grabbing the first date time and the last date time\n",
    "            # but ive now changed it to generic start, so if ya want change it to df['obstime'].iloc[1]\n",
    "            #change end to last date of last year, but thatll blow the data significantly up - I recomend doing \n",
    "            #this after figuring out which weather stations we want coords wise\n",
    "            start = df['obstime'].iloc[1]\n",
    "            print(start)\n",
    "            end = df['obstime'].iloc[-1]\n",
    "            dates = pd.date_range(start = start, end=end, freq='1h')\n",
    "            #we reindex the dataframe with the date range created earlier\n",
    "            #https://stackoverflow.com/questions/49187686/how-to-fill-missing-timestamps-for-time-column-for-a-date-in-pandas\n",
    "            df = df.set_index('obstime').reindex(dates).reset_index()\n",
    "\n",
    "            #https://stackoverflow.com/questions/38134012/pandas-dataframe-fillna-only-some-columns-in-place\n",
    "            #s for synthethised, we pad everything that was NaN values\n",
    "            filename = str(oldFile).split(\"\\\\\")[-1]\n",
    "            stationid = filename.split(\"_\")[0]\n",
    "            paddedDetails = {'station_id':stationid, 'Past 60-Minutes Rainfall in mm':0, 'qcscore':'S'}\n",
    "            df = df.fillna(paddedDetails)\n",
    "            df['station_id'] = df['station_id'].astype(int)\n",
    "\n",
    "            newFileName = Path(oldFile).stem +\"_padded.csv\"\n",
    "            #had a few issues with reset index - using this instead is simpler and quicker to rename the columns\n",
    "            df = df.rename(columns={\"obstime\": \"index\",\"index\":\"obstime\", \"Past 60-Minutes Rainfall in mm\": \"60rf_in_mm\", \"qcscore\": \"qcscore\"})\n",
    "            df.to_csv(\"./preprocessedRFData/\"+newFileName, encoding='utf-8', index=False)\n",
    "        \n",
    "            print(newFileName, \" Completed\")\n",
    "        else:\n",
    "            print(oldFile, \"Ignored\")\n",
    "    print(\"Finished padding data.\")\n",
    "\n",
    "def RHFillOutData():\n",
    "    #first do pathlib to get all the files\n",
    "    directory = \"./RHbyStation\"\n",
    "    files = list(Path(directory).glob('*'))\n",
    "    for oldFile in files:\n",
    "            df = pd.read_csv(oldFile)\n",
    "            df['obstime'] = pd.to_datetime(df['obstime'], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "            \n",
    "            start = df['obstime'].iloc[1]\n",
    "            print(start)\n",
    "            print(df['obstime'].iloc[0])\n",
    "            end = df['obstime'].iloc[-1]\n",
    "            dates = pd.date_range(start = start, end=end, freq='1h')\n",
    "            #we reindex the dataframe with the date range created earlier\n",
    "            #https://stackoverflow.com/questions/49187686/how-to-fill-missing-timestamps-for-time-column-for-a-date-in-pandas\n",
    "            df = df.set_index('obstime').reindex(dates).reset_index()\n",
    "\n",
    "            #https://stackoverflow.com/questions/38134012/pandas-dataframe-fillna-only-some-columns-in-place\n",
    "            #s for synthethised, we pad everything that was NaN values\n",
    "            filename = str(oldFile).split(\"\\\\\")[-1]\n",
    "            stationid = filename.split(\"_\")[0]\n",
    "            paddedDetails = {'station_id':stationid, \"Relative Humidity in %\":0, 'qcscore':'X'}\n",
    "            df = df.fillna(paddedDetails)\n",
    "\n",
    "            newFileName = Path(oldFile).stem +\"_padded.csv\"\n",
    "            #had a few issues with reset index - using this instead is simpler and quicker to rename the columns\n",
    "            df = df.rename(columns={\"obstime\": \"index\",\"index\":\"obstime\", \"Relative Humidity in %\": \"Relative Humidity in %\", \"qcscore\": \"qcscore\"})\n",
    "            df['station_id'] = df['station_id'].astype(int)\n",
    "            df.to_csv(\"./preprocessedRHData/\"+newFileName, encoding='utf-8', index=False)\n",
    "        \n",
    "            print(newFileName, \" Completed\")\n",
    "\n",
    "    print(\"Finished padding data.\")\n",
    "  \n",
    "def filterIntoStations(uniqueStations, datatype, directory, uniqueColumn, fileEnd):\n",
    "    \n",
    "    files = list(Path(directory).glob('*'))\n",
    "    print(\"Started filtering data grouped by station id\")\n",
    "    for station in uniqueStations: \n",
    "        newCSV = [['obstime,station_id,'+uniqueColumn+',qcscore']]\n",
    "        for file in files:\n",
    "            with open(file, 'r') as file:\n",
    "                reader = csv.reader(file, delimiter=',', quotechar= '|')\n",
    "                for row in reader:\n",
    "                    if row[1] == station:\n",
    "                        newCSV.append([row[0], row[1], row[2], row[3]])\n",
    "        newFilename = \"./\"+datatype+\"/\"+station+\"_2011-2023_60\"+fileEnd+\".txt\"\n",
    "        with open(newFilename, 'w') as newFile:\n",
    "                for row in newCSV:\n",
    "                    formatted_row = ','.join(row) + '\\n'\n",
    "                    newFile.write(formatted_row)\n",
    "                print(newFilename, \" Completed\")\n",
    "    print(\"Finished filtering data grouped by station id\")\n",
    "\n",
    "\n",
    "def joinRHRF():\n",
    "    rhDirectory = \"./collatedRHPadded\"\n",
    "    rfDirectory = \"./collatedRFPadded\"\n",
    "    \n",
    "    RHFiles = list(Path(rhDirectory).glob('*'))\n",
    "    RFFiles = list(Path(rfDirectory).glob('*'))\n",
    "\n",
    "    years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "    #https://stackoverflow.com/questions/41815079/pandas-merge-join-two-data-frames-on-multiple-columns\n",
    "    for i in range(len(years)):\n",
    "        RHFile = pd.read_csv(RHFiles[i])\n",
    "        RFFile = pd.read_csv(RFFiles[i])\n",
    "        combinedDF = pd.merge(RFFile, RHFile, how='left', left_on=['obstime', 'station_id'], right_on=['obstime', 'station_id'])\n",
    "        ###Finally pad the data again\n",
    "        paddedDetails = {'Past 60-Minutes Rainfall in mm':0,'qcscore_x':'S',\"Relative Humidity in %\":0, 'qcscore_y':'X'}\n",
    "        combinedDF = combinedDF.fillna(paddedDetails)  \n",
    "        ###Somewhere in the code it converts station id to float, so we're reverting it here just as a quick fix     \n",
    "        combinedDF['station_id'] = combinedDF['station_id'].astype(int)\n",
    "        combinedDF.to_csv(\"./combinedPreprocessed/\"+str(years[i]), encoding='utf-8', index=False)\n",
    "        print(\"Completed combining RF and RH for: \" +str(years[i]))\n",
    "\n",
    "def addHumidity():\n",
    "    uniqueStations = getStationNames(\"./collatedRH\")\n",
    "    print(\"Found these unique weather RH stations: \\n\", uniqueStations)\n",
    "\n",
    "    uniqueStations2 = getStationNames(\"./collatedRF\")\n",
    "    print(\"Found these unique weather RH stations: \\n\", uniqueStations2)\n",
    "    matchupData = []\n",
    "    mismatchData = []\n",
    "    for uniqueStation in uniqueStations:\n",
    "        if uniqueStation not in uniqueStations2:\n",
    "            mismatchData.append(uniqueStation)\n",
    "        if uniqueStation in uniqueStations2:\n",
    "            matchupData.append(uniqueStation)\n",
    "    print(\"\\n MatchUps: \\n\")\n",
    "    print(matchupData)\n",
    "    print(\"\\n Mismatchs: \\n\")\n",
    "    print(mismatchData)\n",
    "\n",
    "    ###Need to split RH into by weather station\n",
    "    filterIntoStations(uniqueStations,\"RHbyStation\", \"./collatedRH\", \"Relative Humidity in %\", \"rh\")\n",
    "    ###Then need too pad and save the data\n",
    "    RHFillOutData()\n",
    "    FillOutDataYear(\"RH\")\n",
    "\n",
    "    ###Then need to conditionally check where the data matches up and link it\n",
    "    joinRHRF()\n",
    "\n",
    "def getStationCoords(directory):\n",
    "    files = Path(directory).glob('*')\n",
    "    station_coords = {}\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        stationData = df[['station_id', 'latitude', 'longitude']]\n",
    "        for index, row in stationData.iterrows():\n",
    "            station_id = row['station_id']\n",
    "            latitude = row['latitude']\n",
    "            longitude = row['longitude']\n",
    "            if station_id not in station_coords:\n",
    "                station_coords[station_id] = [longitude, latitude]\n",
    "\n",
    "    with open(\"station_coords.json\", 'w') as json_file:\n",
    "        json.dump(station_coords, json_file)\n",
    "    print(\"Finished getting stations\")\n",
    "\n",
    "\n",
    "\n",
    "def dropQScores():\n",
    "    rhDirectory = \"./collatedRHPadded\"\n",
    "    rfDirectory = \"./collatedRFPadded\"\n",
    "    combinedDirectory = \"./combinedPreprocessed\"\n",
    "\n",
    "\n",
    "    RHFiles = list(Path(rhDirectory).glob('*'))\n",
    "    RFFiles = list(Path(rfDirectory).glob('*'))\n",
    "    combinedFiles = list(Path(combinedDirectory).glob('*'))\n",
    "\n",
    "    #for file in RHFiles:\n",
    "    #    df = pd.read_csv(file)\n",
    "    #    df.drop(columns=['qcscore'], inplace=True)\n",
    "    #    df.to_csv(file, encoding='utf-8', index=False)\n",
    "#\n",
    "    #for file in RFFiles:\n",
    "    #    df = pd.read_csv(file)\n",
    "    #    df.drop(columns=['qcscore'], inplace=True)\n",
    "    #    df.to_csv(file, encoding='utf-8', index=False)\n",
    "    \n",
    "    for file in combinedFiles:\n",
    "        df = pd.read_csv(file)\n",
    "        df.drop(columns=['qcscore_x', 'qcscore_y'], inplace=True)\n",
    "        df.to_csv(file, encoding='utf-8', index=False)\n",
    "\n",
    "#https://www.geeksforgeeks.org/how-to-iterate-over-files-in-directory-using-python/\n",
    "def getStationNames():\n",
    "    files = Path(\"./collatedRFPadded\").glob('*')\n",
    "    uniqueStations = []\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        with open(file, 'r') as file:\n",
    "            reader = csv.reader(file, delimiter=',', quotechar='|')\n",
    "            for row in reader:\n",
    "                station = row[1]\n",
    "                if row[1] not in uniqueStations:\n",
    "                    uniqueStations.append(row[1])\n",
    "        #get rid of station_id header\n",
    "    del uniqueStations[0]\n",
    "    print(\"Finished getting unique station names\")\n",
    "    return uniqueStations\n",
    "\n",
    "def FillOutDataYear(datatype, year):\n",
    "    #first do pathlib to get all the files\n",
    "    #Issue is that \n",
    "    #years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "    \n",
    "    directory = \"./preprocessed\"+datatype+\"Data\"\n",
    "    files = list(Path(directory).glob('*'))\n",
    "    finalYearDF = pd.DataFrame()\n",
    "    #Directory is by weather station, so need to iterate through each file and where the year \n",
    "    for oldFile in files:\n",
    "        #6993 is just one entry long so we ignore\n",
    "        #https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values\n",
    "        df = pd.read_csv(oldFile)\n",
    "            \n",
    "        #just in case this isnt datetime\n",
    "        #df['obstime'] = pd.to_datetime(df['obstime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        df['obstime'] = pd.to_datetime(df['obstime'], format=\"ISO8601\")\n",
    "        #df['obstime'] = addT(df['obstime'])\n",
    "        #create year column\n",
    "        df['year'] = df['obstime'].dt.year\n",
    "        #dataframe that includes just the year\n",
    "        \n",
    "        yearDF = df[df['year'] == year]\n",
    "\n",
    "        \n",
    "        finalYearDF = pd.concat([finalYearDF, yearDF], ignore_index=True)\n",
    "        #print(Path(oldFile).stem, \" Completed\")\n",
    "        finalYearDF.drop(columns=['year'], inplace=True)\n",
    "        newFileName = str(year)+ \"_Year_\"+datatype+\"_padded.csv\"\n",
    "        finalYearDF['station_id'] = finalYearDF['station_id'].astype(int)\n",
    "        #print(\"Completed: \", oldFile)\n",
    "    finalYearDF.to_csv(\"./collated\"+datatype+\"Padded/\"+newFileName, encoding='utf-8', index=False)\n",
    "    print(\"Finished padding data.\")\n",
    "\n",
    "def fixTime():\n",
    "    dataDir = './collatedRFPadded'\n",
    "    files = list(Path(dataDir).glob('*'))\n",
    "    #apply the removepunctuation function to the text column\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['obstime'] = df['obstime'].apply(addT)\n",
    "        display(df)\n",
    "        df.to_csv(str(file), encoding='utf-8', index=False)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def addT(obstime): \n",
    "    #loop through all punctuation marks and replace them with empty string\n",
    "    obstime = obstime.replace(\" \", \"T\")\n",
    "    return obstime\n",
    "\n",
    "#uniqueStations = getStationNames()\n",
    "#print(\"Found these unique weather stations: \\n\", uniqueStations)\n",
    "#filterIntoStations(uniqueStations, \"RFbyStation\",\"./collatedRF\", \"Past 60-Minutes Rainfall in mm\", \"rf\")\n",
    "#newFillOutData()\n",
    "#FillOutDataYear(\"RF\", 2023)\n",
    "#PuthearaKeFunction()\n",
    "# addHumidity()\n",
    "\n",
    "##Only run this if you want to drop the q scores\n",
    "# dropQScores()\n",
    "\n",
    "#fixTime()\n",
    "\n",
    "test1 = ['1005', '1010', '1012', '1013', '1016', '1017', '6001', '6002', '6003', '6007', '6009', '6015', '6016', '6017', '6018', '6025', '6028', '6029', '6031', '6034', '6040', '6041', '6042', '6043', '6045', '6047', '6049', '6053', '6055', '6056', '6058', '6061', '6021', '6027', '6020', '6036', '6051', '6013', '6039', '6037', '6026', '6046', '6101', '6048', '6998', '6060', '6063', '6054', '6062', '6059', '6065', '6993', '6064', '6999', '6067', '6068', '6066', '6070', '6069', '6050', '6071', '6072', '6077', '6075', '6074', '6076', '6079', '6080', '6081', '6102', '6103', '6082', '6078', '6084', '6083', '6085', '6011', '6086', '6087', '6088', '6089', '6092', '6104', '6093', '6094', '6095', '6090', '6097', '6096', '6091', '6100', '6106', '6098', '6107', '6108', '6111', '6995', '6109', '6112', '6116', '6120', '6117', '6121', '6115', '6119', '6123', '6124', '6005', '6125', '6126', '9913', '6127', '6128', '6129', '6130', '6133', '6131', '6132', '6134', '6135', '6136', '6110', '6138', '6139', '6144', '6143', '6142', '6147', '6149']\n",
    "test2 =  ['1005', '1010', '1012', '1013', '1016', '1017', '6001', '6002', '6003', '6007', '6009', '6013', '6015', '6016', '6017', '6018', '6020', '6021', '6025', '6026', '6027', '6028', '6029', '6031', '6034', '6036', '6037', '6039', '6040', '6041', '6042', '6043', '6045', '6046', '6047', '6048', '6049', '6051', '6053', '6054', '6055', '6056', '6058', '6059', '6060', '6061', '6062', '6063', '6064', '6065', '6066', '6067', '6068', '6069', '6070', '6101', '6998', '6999', '6050', '6071', '6072', '6074', '6075', '6076', '6077', '6079', '6080', '6081', '6102', '6103', '6078', '6082', '6083', '6084', '6011', '6085', '6086', '6087', '6088', '6089', '6092', '6090', '6091', '6093', '6094', '6095', '6096', '6097', '6104', '6098', '6100', '6106', '6107', '6108', '6111', '6995', '6109', '6112', '6116', '6115', '6117', '6119', '6120', '6121', '6123', '6124', '6005', '6125', '6126', '6127', '6128', '9913', '6129', '6130', '6131', '6132', '6133', '6134', '6110', '6135', '6136', '6138', '6139', '6142', '6143', '6144', '6147', '6149']\n",
    "\n",
    "for station in test2:\n",
    "    if station not in test1:\n",
    "        print(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0ff76-a13e-477c-bb47-de76a0eebffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
